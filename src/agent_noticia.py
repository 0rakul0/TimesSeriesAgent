import os
import pandas as pd
import json
from babel.dates import format_date
from datetime import datetime
from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from tavily import TavilyClient
from dotenv import load_dotenv
import os

# Carrega vari√°veis do .env
load_dotenv()


LIMIAR_VARIACAO = 5.0  # % limite para disparar busca de not√≠cias
CAMINHO_DADOS = "../data/dados_combinados.csv"
CAMINHO_SAIDA_EVENTOS = "../ouput_noticias"


class ResumoNoticias(BaseModel):
    data: str = Field(..., description="Data do evento analisado")
    contexto: str = Field(..., description="Contexto pol√≠tico e econ√¥mico do evento")
    acontecimento: str = Field(..., description="O que ocorreu com a Petrobras")
    impacto: str = Field(..., description="Impacto sobre a empresa e o mercado")
    fontes: str = Field(..., description="Principais fontes de informa√ß√£o (se houver)")


# ======================================
# 1Ô∏è‚É£ Configura√ß√£o do cliente tavily e LLM
# ======================================
# L√™ a chave da Tavily
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

tavily_client = TavilyClient(api_key=TAVILY_API_KEY)
llm = ChatOllama(model="llama3.2", base_url="http://localhost:11434")


def resumir_trecho(trecho: str, max_tokens: int = 100):
    prompt = ChatPromptTemplate.from_template("""
    Resuma o seguinte trecho de not√≠cia em at√© {max_tokens} tokens, mantendo os pontos principais:
    {input}
    """)
    chain = prompt | llm
    response = chain.invoke({"input": trecho, "max_tokens": max_tokens})
    return response.content

# ======================================
# üïµÔ∏è 1Ô∏è‚É£ Agente Coletor (usando Tavily)
# ======================================
def coletar_noticias(ativo, data_evento: str):
    """
    Coleta e resume not√≠cias relevantes para o ativo (PETR4 ou Brent) na data especificada.
    """
    if ativo == "PETR4":
        procura = "Petrobras PETR4"
    else:
        procura = "pre√ßo do petr√≥leo Brent"

    query = f"not√≠cias sobre {procura} em {data_evento}"
    print(f"üóûÔ∏è Buscando not√≠cias: {query}")

    resp = tavily_client.search(
        query=query,
        start_date=data_evento,
        search_depth="advanced",
        max_results=3,
        include_answer=True,
    )

    if not resp.get("results"):
        return "Nenhuma not√≠cia encontrada."

    noticias = []
    for doc in resp["results"]:
        titulo = doc.get("title", "Sem t√≠tulo")
        url = doc.get("url", "")
        trecho = doc.get("content", "")

        # ‚úÖ Resumo autom√°tico do trecho
        resumo_trecho = resumir_trecho(trecho)

        noticias.append({
            "titulo": titulo,
            "url": url,
            "resumo": resumo_trecho
        })

    # Retorna formato limpo e compacto
    return "\n\n".join(
        f"T√≠tulo: {n['titulo']}\nLink: {n['url']}\nResumo: {n['resumo']}"
        for n in noticias
    )

# ======================================
# üòä 2Ô∏è‚É£ Agente Analisador de Sentimentos
# ======================================
def analisar_sentimento(texto: str, preco_atual: float, data: str):
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", f"""
        Voc√™ √© um analista financeiro especializado no mercado de a√ß√µes brasileiro.
        Analise as not√≠cias sobre a Petrobras (PETR4) publicadas em {data} e:
        1Ô∏è‚É£ Classifique o sentimento geral (POSITIVO, NEGATIVO ou NEUTRO)
        2Ô∏è‚É£ Estime o impacto percentual no pre√ßo do ativo
        3Ô∏è‚É£ Projete o pre√ßo futuro considerando o pre√ßo atual de R${preco_atual:.2f}
        4Ô∏è‚É£ Justifique brevemente o racioc√≠nio.
        Formato:
        ---
        Sentimento: <positivo|negativo|neutro>
        Impacto estimado: <percentual>
        Pre√ßo projetado: R$<valor>
        Justificativa: <texto curto>
        ---
        """),
        ("user", "{input}")
    ])
    chain = prompt_template | llm
    response = chain.invoke({"input": texto})
    return response.content


# ======================================
# üßæ 3Ô∏è‚É£ Agente Resumidor
# ======================================
def resumir_noticias(texto: str, data: str):
    prompt = ChatPromptTemplate.from_template("""
    Voc√™ √© um jornalista econ√¥mico especializado em mercado financeiro brasileiro.
    Resuma as not√≠cias sobre a Petrobras (PETR4) em {data} no formato JSON:
    {{
        "data": "{data}",
        "contexto": "<descri√ß√£o do contexto pol√≠tico e econ√¥mico>",
        "acontecimento": "<o que aconteceu>",
        "impacto": "<impacto sobre a empresa e o mercado>",
        "fontes": "<principais fontes citadas>"
    }}
    Not√≠cias:
    {input}
    """)
    chain = prompt | llm.with_structured_output(ResumoNoticias)
    response = chain.invoke({"input": texto, "data": data})
    return response

def detectar_eventos(caminho_csv: str = CAMINHO_DADOS, limiar: float = LIMIAR_VARIACAO):
    df = pd.read_csv(caminho_csv, index_col=0, parse_dates=True)
    df.index = pd.to_datetime(df.index).normalize()

    col_petr = "Close_PETR4.SA"
    col_brent = "Close_BZ=F"

    # ‚úÖ Calcula varia√ß√µes percentuais
    df["Ret_PETR4"] = df[col_petr].pct_change() * 100
    df["Ret_BZ"] = df[col_brent].pct_change() * 100

    # üîç Detecta eventos relevantes
    eventos_petr = df[abs(df["Ret_PETR4"]) > limiar].copy()
    eventos_petr["origem_evento"] = "PETR4"

    eventos_brent = df[abs(df["Ret_BZ"]) > limiar].copy()
    eventos_brent["origem_evento"] = "Brent"

    # Junta e trata duplicatas (eventos simult√¢neos)
    eventos = pd.concat([eventos_petr, eventos_brent]).sort_index()

    # Se no mesmo dia ocorrer evento em ambos, marca como ‚ÄúAmbos‚Äù
    eventos = (
        eventos.groupby(eventos.index)
        .agg({
            col_petr: "first",
            col_brent: "first",
            "Ret_PETR4": "first",
            "Ret_BZ": "first",
            "origem_evento": lambda x: "Ambos" if len(set(x)) > 1 else list(x)[0],
        })
    )

    if eventos.empty:
        print("‚úÖ Nenhum evento relevante encontrado.")
        return

    registros = []
    os.makedirs(CAMINHO_SAIDA_EVENTOS, exist_ok=True)

    for data_evento, linha in eventos.iterrows():
        preco_petr = linha[col_petr]
        preco_brent = linha[col_brent]
        ret_petr = linha["Ret_PETR4"]
        ret_brent = linha["Ret_BZ"]
        origem = linha["origem_evento"]

        # üóìÔ∏è Formata a data no estilo "22 de fevereiro de 2021"
        data_formatada = format_date(data_evento, format="d 'de' MMMM 'de' y", locale='pt_BR')

        print(f"\nüö® Evento detectado em {data_formatada} ({origem}):")
        print(f"   ‚Ä¢ PETR4: varia√ß√£o de {ret_petr:.2f}% (pre√ßo: R${preco_petr:.2f})")
        print(f"   ‚Ä¢ Brent: varia√ß√£o de {ret_brent:.2f}% (pre√ßo: US${preco_brent:.2f})")

        # 1Ô∏è‚É£ Coleta de not√≠cias
        if origem == "PETR4":
            noticias = coletar_noticias("PETR4", data_evento.strftime("%Y-%m-%d"))
        elif origem == "Brent":
            noticias = coletar_noticias("Brent", data_evento.strftime("%Y-%m-%d"))
        else:  # origem == "Ambos"
            noticias_petr = coletar_noticias("PETR4", data_evento.strftime("%Y-%m-%d"))
            noticias_brent = coletar_noticias("Brent", data_evento.strftime("%Y-%m-%d"))
            noticias = {
                "Petrobras": noticias_petr,
                "Brent": noticias_brent
            }

        # 2Ô∏è‚É£ An√°lise de sentimento
        analise = analisar_sentimento(noticias, preco_petr, data_formatada)

        # 3Ô∏è‚É£ Resumo (pode retornar objeto BaseModel)
        resumo = resumir_noticias(noticias, data_formatada)
        resumo_dict = resumo.model_dump() if hasattr(resumo, "model_dump") else {"resumo": resumo}

        # 4Ô∏è‚É£ Registro estruturado
        registro = {
            "data": data_evento.strftime("%Y-%m-%d"),
            "data_formatada": data_formatada,
            "origem_evento": origem,
            "preco_petr4": preco_petr,
            "preco_brent": preco_brent,
            "variacao_petr4": ret_petr,
            "variacao_brent": ret_brent,
            "analise": analise,
            "noticias": noticias,
            **resumo_dict
        }
        registros.append(registro)

        # üíæ Salva JSON parcial por rodada
        nome_arquivo_json = os.path.join(
            CAMINHO_SAIDA_EVENTOS,
            f"evento_{data_evento.strftime('%Y-%m-%d')}_{origem}.json"
        )
        with open(nome_arquivo_json, "w", encoding="utf-8") as f:
            json.dump(registro, f, ensure_ascii=False, indent=4)
        print(f"üíæ Evento salvo: {nome_arquivo_json}")

    # üíæ Salvar CSV final consolidado
    df_eventos = pd.DataFrame(registros)
    caminho_csv_final = os.path.join(CAMINHO_SAIDA_EVENTOS, "eventos_consolidados.csv")
    df_eventos.to_csv(caminho_csv_final, index=False)
    print(f"\n‚úÖ Eventos consolidados salvos em {caminho_csv_final}")

# ======================================
# üöÄ Fun√ß√£o Principal
# ======================================
def main():
    detectar_eventos()


if __name__ == "__main__":
    main()
